# -*- coding: utf-8 -*-
"""GPT_2CORD19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aIIlfCLWEvqF3BuF-x4_KNRwHllMxDcx

# ***Link:*** [CORD Dataset](https://www.kaggle.com/datasets/allen-institute-for-ai/CORD-19-research-challenge)
"""

from google.colab import drive
drive.mount('/content/drive')

pip install transformers

import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/drive/My Drive/metadata.csv') #Change to local directory
df = df['abstract'].dropna()
df_sample = df.sample(n=25000)

tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')

tokenizer.pad_token = tokenizer.eos_token

tokenized_data = [tokenizer.encode(abstract, truncation=True, max_length=512) for abstract in df_sample]

train_data, val_data = train_test_split(tokenized_data, test_size=0.1)

class CORD19Dataset(Dataset):
    def __init__(self, tokenized_data):
        self.tokenized_data = tokenized_data

    def __len__(self):
        return len(self.tokenized_data)

    def __getitem__(self, idx):
        return torch.tensor(self.tokenized_data[idx])

def collate_fn(batch):
    padded_sequences = pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)
    return padded_sequences

train_dataset = CORD19Dataset(train_data)
val_dataset = CORD19Dataset(val_data)

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)

model = GPT2LMHeadModel.from_pretrained('gpt2-medium').cuda()
optimizer = AdamW(model.parameters(), lr=5e-5)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

accumulation_steps = 4

for epoch in range(1):
    model.train()
    total_loss = 0
    for batch in train_loader:
        inputs = batch.to(device)
        outputs = model(inputs, labels=inputs)
        loss = outputs.loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch} - Training loss: {total_loss / len(train_loader)}")

    model.eval()
    total_eval_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            inputs = batch.to(device)
            outputs = model(inputs, labels=inputs)
            loss = outputs.loss
            total_eval_loss += loss.item()

    print(f"Epoch {epoch} - Validation loss: {total_eval_loss / len(val_loader)}")

model.save_pretrained('/content/drive/My Drive/gpt2-medium-finetuned-cord19')

model.eval()
prompt_text = "Recent studies on COVID-19 suggest that"

input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)

output_sequences = model.generate(
    input_ids=input_ids,
    max_length=100,
    temperature=1.0,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.2,
    do_sample=True,
    num_return_sequences=1
)

generated_sequence = tokenizer.decode(output_sequences[0], skip_special_tokens=True)
print(generated_sequence)