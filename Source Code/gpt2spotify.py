# -*- coding: utf-8 -*-
"""GPT2Spotify.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1afGYaRm84ttmBVLm5jQhBtTfoLONejPg

# ***Link:*** [Spotify Dataset](https://www.kaggle.com/datasets/joebeachcapital/30000-spotify-songs)
"""

from google.colab import drive
drive.mount('/content/drive')

pip install transformers

import torch
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import TensorDataset
from torch.nn.utils.rnn import pad_sequence
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/drive/My Drive/spotifysongs.csv') #Change to Local Directory
columns = ['track_name', 'track_artist', 'track_album_name',
           'playlist_genre', 'playlist_subgenre',
           'danceability', 'energy', 'valence', 'tempo']
df = df[columns].dropna()
print (df)

def normalize_and_concatenate(row):
    numerical_data = ' '.join([f"{col}:{row[col]:.2f}" if isinstance(row[col], (int, float)) else f"{col}:{row[col]}" for col in columns[4:]])
    return ' '.join(row[:4].values) + ' ' + numerical_data

df['input_data'] = df.apply(normalize_and_concatenate, axis=1)
df_sample = df['input_data'].sample(n=1000)

tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
tokenizer.pad_token = tokenizer.eos_token

tokenized_data = [tokenizer.encode(data, truncation=True, max_length=512) for data in df['input_data']]

train_data, val_data = train_test_split(tokenized_data, test_size=0.1)

class SpotifyDataset(Dataset):
    def __init__(self, tokenized_data):
        self.tokenized_data = tokenized_data

    def __len__(self):
        return len(self.tokenized_data)

    def __getitem__(self, idx):
        return torch.tensor(self.tokenized_data[idx])

def collate_fn(batch):
    padded_sequences = pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)
    return padded_sequences

train_dataset = SpotifyDataset(train_data)
val_dataset = SpotifyDataset(val_data)

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)

model = GPT2LMHeadModel.from_pretrained('gpt2-medium').cuda()
optimizer = AdamW(model.parameters(), lr=5e-5)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for epoch in range(1):
    model.train()
    total_loss = 0
    for batch in train_loader:
        inputs = batch.to(device)
        outputs = model(inputs, labels=inputs)
        loss = outputs.loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch} - Training loss: {total_loss / len(train_loader)}")

    model.eval()
    total_eval_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            inputs = batch.to(device)
            outputs = model(inputs, labels=inputs)
            loss = outputs.loss
            total_eval_loss += loss.item()

    print(f"Epoch {epoch} - Validation loss: {total_eval_loss / len(val_loader)}")

model.save_pretrained('/content/drive/My Drive/gpt2-medium-finetuned-spotify')

tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
model = GPT2LMHeadModel.from_pretrained('/content/drive/My Drive/gpt2-medium-finetuned-spotify')
model.eval()

genre_prompt = "Genre: Pop. Theme: Love and Heartbreak. Lyrics:"

def generate_lyrics(prompt):
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    max_length = len(input_ids[0]) + 50

    outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=1)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return generated_text[len(prompt):]

lyrics = generate_lyrics(genre_prompt)
print("Generated Lyrics:\n", lyrics)

def generate_lyrics(prompt):
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    max_length = len(input_ids[0]) + 50

    outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=1, temperature=0.8)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return generated_text[len(prompt):]

genre_prompt = "Genre: Pop. Theme: Love and Heartbreak. Lyrics:"
lyrics = generate_lyrics(genre_prompt)
print("Generated Lyrics:\n", lyrics)